{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link href='http://fonts.googleapis.com/css?family=Alegreya+Sans:100,300,400,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Arvo:400,700,400italic' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=PT+Mono' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Shadows+Into+Light' rel='stylesheet' type='text/css'>\n",
       "<link href='http://fonts.googleapis.com/css?family=Philosopher:400,700,400italic,700italic' rel='stylesheet' type='text/css'>\n",
       "\n",
       "<style>\n",
       "\n",
       "@font-face {\n",
       "    font-family: \"Computer Modern\";\n",
       "    src: url('http://mirrors.ctan.org/fonts/cm-unicode/fonts/otf/cmunss.otf');\n",
       "}\n",
       "\n",
       ".container { width: 100% }\n",
       "\n",
       "/* Formatting for header cells */\n",
       ".text_cell_render h1 {\n",
       "    font-family: 'Philosopher', sans-serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 2.2em;\n",
       "    line-height: 100%;\n",
       "    color: rgb(0, 80, 120);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.1em;\n",
       "    display: block;\n",
       "}\t\n",
       ".text_cell_render h2 {\n",
       "    font-family: 'Philosopher', serif;\n",
       "    font-weight: 400;\n",
       "    font-size: 1.9em;\n",
       "    line-height: 100%;\n",
       "    color: rgb(200,100,0);\n",
       "    margin-bottom: 0.1em;\n",
       "    margin-top: 0.1em;\n",
       "    display: block;\n",
       "}\t\n",
       "\n",
       ".text_cell_render h3 {\n",
       "    font-family: 'Philosopher', serif;\n",
       "    margin-top:12px;\n",
       "    margin-bottom: 3px;\n",
       "    font-style: italic;\n",
       "    color: rgb(94,127,192);\n",
       "}\n",
       "\n",
       ".text_cell_render h4 {\n",
       "    font-family: 'Philosopher', serif;\n",
       "}\n",
       "\n",
       ".text_cell_render h5 {\n",
       "    font-family: 'Alegreya Sans', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 16pt;\n",
       "    color: grey;\n",
       "    font-style: italic;\n",
       "    margin-bottom: .1em;\n",
       "    margin-top: 0.1em;\n",
       "    display: block;\n",
       "}\n",
       "\n",
       ".text_cell_render h6 {\n",
       "    font-family: 'PT Mono', sans-serif;\n",
       "    font-weight: 300;\n",
       "    font-size: 10pt;\n",
       "    color: grey;\n",
       "    margin-bottom: 1px;\n",
       "    margin-top: 1px;\n",
       "}\n",
       "\n",
       ".text_cell_render em {\n",
       "    font-family: 'Philosopher', sans-serif;\n",
       "    color:        blue;\n",
       "    background-color: rgb(255,220,180);\n",
       "    font-size:    110%;\n",
       "    margin-left:   2px;\n",
       "    margin-right:  2px;\n",
       "    font-weight:   100;\n",
       "}\n",
       "\n",
       ".text_cell_render b {\n",
       "    color:            rgb(255,195,195);\n",
       "    background-color: rgb(0,0,0);\n",
       "    font-size:    110%;\n",
       "    margin-left:   2px;\n",
       "    margin-right:  2px;\n",
       "    font-weight:   650;\n",
       "}\n",
       "\n",
       ".text_cell_render tt {\n",
       "    font-size:    120%;\n",
       "    margin-left:   2px;\n",
       "    margin-right:  2px;\n",
       "    font-weight:   150;\n",
       "}\n",
       "\n",
       ".Codemirror {\n",
       "    font-family: \"PT Mono\";\n",
       "    font-size: 100%;\n",
       "}\n",
       "\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "with open('../../style.css', 'r') as file:\n",
    "    css = file.read()\n",
    "HTML(css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Earley Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Grammar for Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earley's algorithm has tow inputs:\n",
    "- a grammar $G$ and\n",
    "- a string $s$.\n",
    "\n",
    "It then checks whether the string $s$ can be parsed with the given grammar.\n",
    "\n",
    "In order to input the grammar in a natural way, we first have to develop a parser for grammars.\n",
    "An example grammar that we want to parse is stored in the file `simple.g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expr: expr '+' product;\r\n",
      "expr: product;\r\n",
      "    \r\n",
      "product: product '*' factor;\r\n",
      "product: factor;\r\n",
      "       \r\n",
      "factor: '(' expr ')';\r\n",
      "factor: NUMBER;\r\n"
     ]
    }
   ],
   "source": [
    "!cat simple.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use <span style=\"font-variant:small-caps;\">Antlr</span> to develop a parser for this Grammar.  \n",
    "The pure grammar to parse this type of grammar is stored in\n",
    "the file `Pure.g4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar Grammar;\r\n",
      "\r\n",
      "start: grmrl+;\r\n",
      "\r\n",
      "grmrl: VARIABLE ':' item+ ;\r\n",
      " \r\n",
      "item : VARIABLE \r\n",
      "     | TOKEN  \r\n",
      "     | LITERAL\r\n",
      "     ;\r\n",
      "\r\n",
      "VARIABLE: [a-z]+;\r\n",
      "TOKEN   : [A-Z]+;\r\n",
      "LITERAL : '\\'' ~('\\'')+ '\\'';\r\n",
      "        \r\n",
      "WS      : [ \\t\\n\\r] -> skip ;\r\n"
     ]
    }
   ],
   "source": [
    "!cat Pure.g4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated grammar is stored in the file `Grammar.g4`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\tgrammar Grammar;\r\n",
      "     2\t\r\n",
      "     3\tstart returns [g]\r\n",
      "     4\t    : {rules = []}\r\n",
      "     5\t      (r=grmrl {rules.append($r.r)})+\r\n",
      "     6\t      {$g = rules}\r\n",
      "     7\t    ;\r\n",
      "     8\t\r\n",
      "     9\tgrmrl returns [r]\r\n",
      "    10\t    : {body = []}\r\n",
      "    11\t      v=VARIABLE ':' (i=item {body.append($i.atom)})+ ';' \r\n",
      "    12\t      {$r = ($v.text,) + tuple(body)}\r\n",
      "    13\t    ;\r\n",
      "    14\t \r\n",
      "    15\titem returns [atom]\r\n",
      "    16\t    : v=VARIABLE {$atom = $v.text}\r\n",
      "    17\t    | t=TOKEN    {$atom = $t.text}\r\n",
      "    18\t    | l=LITERAL  {$atom = $l.text}\r\n",
      "    19\t    ;\r\n",
      "    20\t\r\n",
      "    21\tVARIABLE: [a-z]+;\r\n",
      "    22\tTOKEN   : [A-Z]+;\r\n",
      "    23\tLITERAL : '\\'' ~('\\'')+ '\\'';\r\n",
      "    24\t        \r\n",
      "    25\tWS      : [ \\t\\n\\r] -> skip ;\r\n"
     ]
    }
   ],
   "source": [
    "!cat -n Grammar.g4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by generating both scanner and parser.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!antlr4 -Dlanguage=Python3 Grammar.g4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarLexer  import GrammarLexer\n",
    "from GrammarParser import GrammarParser\n",
    "import antlr4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `parse_grammar` takes a `filename` as its argument and returns the grammar that is stored in the given file.  The grammar is represented as list of rules.  Each rule is represented as a tuple.  The example below will clarify this structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_grammar(filename):\n",
    "    input_stream  = antlr4.FileStream(filename)\n",
    "    lexer         = GrammarLexer(input_stream)\n",
    "    token_stream  = antlr4.CommonTokenStream(lexer)\n",
    "    parser        = GrammarParser(token_stream)\n",
    "    grammar       = parser.start()\n",
    "    return grammar.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expr', 'expr', \"'+'\", 'product'),\n",
       " ('expr', 'product'),\n",
       " ('product', 'product', \"'*'\", 'factor'),\n",
       " ('product', 'factor'),\n",
       " ('factor', \"'('\", 'expr', \"')'\"),\n",
       " ('factor', 'NUMBER')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_grammar('simple.g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earley's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a context-free $G = \\langle V, \\Sigma, R, S \\rangle$ and a string $s = x_1x_2 \\cdots x_n \\in \\Sigma^*$ of length $n$, \n",
    "an *Earley item* is a pair of the form\n",
    "$$\\langle A \\rightarrow \\alpha \\bullet \\beta, k \\rangle$$\n",
    "such that \n",
    "- $(A \\rightarrow \\alpha \\beta) \\in R\\quad$  and\n",
    "- $k \\in \\{0,1,\\cdots,n\\}$. \n",
    "\n",
    "The class `EarleyItem` represents a single *Earley item*.  \n",
    "- `mVariable` is the variable $A$,\n",
    "- `mAlpha` is $\\alpha$,\n",
    "- `mBeta` is $\\beta$, and\n",
    "- `mIndex` is $k$.\n",
    "\n",
    "Since we later have to store objects of class `EarleyItem` in sets, we have to implement the functions\n",
    "- `__eq__`,\n",
    "- `__ne__`,\n",
    "- `__hash__`.\n",
    "\n",
    "It is easiest to implement `__hash__` by first converting the object into a string.  Hence we also\n",
    "implement the function `__repr__`, that converts an `EarleyItem` into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyItem():\n",
    "    def __init__(self, variable, alpha, beta, index):\n",
    "        self.mVariable = variable\n",
    "        self.mAlpha    = alpha\n",
    "        self.mBeta     = beta\n",
    "        self.mIndex    = index\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, EarleyItem)     and \\\n",
    "               self.mVariable == other.mVariable and \\\n",
    "               self.mAlpha    == other.mAlpha    and \\\n",
    "               self.mBeta     == other.mBeta     and \\\n",
    "               self.mIndex    == other.mIndex\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        alphaStr = ' '.join(self.mAlpha)\n",
    "        betaStr  = ' '.join(self.mBeta)\n",
    "        return f'<{self.mVariable} → {alphaStr} • {betaStr}, {self.mIndex}>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an Earley item `self`, the function `isComplete` checks, whether the Earley item `self` has the form\n",
    "$$\\langle A \\rightarrow \\alpha \\bullet, k \\rangle,$$\n",
    "i.e. whether the $\\bullet$ is at the end of the grammar rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isComplete(self):\n",
    "    return self.mBeta == ()\n",
    "\n",
    "EarleyItem.isComplete = isComplete\n",
    "del isComplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `sameVar(self, C)` checks, whether the item following the dot is the same as the variable \n",
    "given as argument, i.e. `sameVar(self, C)` returns `True` if `self` is an Earley item of the form\n",
    "$$\\langle A \\rightarrow \\alpha \\bullet C\\beta, k \\rangle.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sameVar(self, C):\n",
    "    return len(self.mBeta) > 0 and self.mBeta[0] == C\n",
    "\n",
    "EarleyItem.sameVar = sameVar\n",
    "del sameVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `scan(self, t)` checks, whether the item following the dot matches the token `t`, \n",
    "i.e. `sameVar(self, C)` returns `True` if `self` is an Earley item of the form\n",
    "$$\\langle A \\rightarrow \\alpha \\bullet t\\beta, k \\rangle.$$\n",
    "The argument $t$ can either be the name of a token or a literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(self, t):\n",
    "    if len(self.mBeta) > 0:\n",
    "        return self.mBeta[0] == t or self.mBeta[0] == \"'\" + t + \"'\"\n",
    "    return False\n",
    "\n",
    "EarleyItem.scan = scan\n",
    "del scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an Earley item, this function returns the name of the variable following the dot.  If there is no variable following the dot, the function returns `None`.  The function can distinguish variables from token names because variable names consist only of lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextVar(self):\n",
    "    if len(self.mBeta) > 0:\n",
    "        var = self.mBeta[0]\n",
    "        if var[0] != \"'\" and var.islower():\n",
    "            return var\n",
    "    return None\n",
    "\n",
    "EarleyItem.nextVar = nextVar\n",
    "del nextVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `moveDot(self)` moves the $\\bullet$ in the Earley item `self`, where `self` has the form \n",
    "$$\\langle A \\rightarrow \\alpha \\bullet \\beta, k \\rangle$$\n",
    "over the next variable, token, or literal in $\\beta$.  It assumes that $\\beta$ is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveDot(self):\n",
    "    return EarleyItem(self.mVariable, \n",
    "                      self.mAlpha + (self.mBeta[0],), \n",
    "                      self.mBeta[1:], \n",
    "                      self.mIndex)\n",
    "\n",
    "EarleyItem.moveDot = moveDot\n",
    "del moveDot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Grammar` represents a context free grammar.  It stores a list of the rules of the grammar.\n",
    "Each grammar rule of the form\n",
    "$$ a \\rightarrow \\beta $$\n",
    "is stored as the tuple $(a,) + \\beta$.  The start symbol is assumed to be the variable on the left hand side of\n",
    "the first rule. To distinguish syntactical variables form tokens, variables contain only lower case letters,\n",
    "while tokens either contain only upper case letters or they start and end with a single quote character \"`'`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar():\n",
    "    def __init__(self, Rules):\n",
    "        self.mRules = Rules   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `startItem` returns the Earley item\n",
    "$$ \\langle\\hat{S} \\rightarrow \\bullet S, 0\\rangle $$\n",
    "where $S$ is the start variable of the given grammar and $\\hat{S}$ is a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startItem(self):\n",
    "    return EarleyItem('Start', (), (self.startVar(),), 0)\n",
    "\n",
    "Grammar.startItem = startItem\n",
    "del startItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `finishItem` returns the Earley item\n",
    "$$ \\langle\\hat{S} \\rightarrow S \\bullet, 0\\rangle $$\n",
    "where $S$ is the start variable of the given grammar and $\\hat{S}$ is a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishItem(self):\n",
    "    return EarleyItem('Start', (self.startVar(),), (), 0)\n",
    "\n",
    "Grammar.finishItem = finishItem\n",
    "del finishItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `startVar` returns the start variable of the grammar.  It is assumed that\n",
    "the first rule grammar starts with the start variable of the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startVar(self):\n",
    "    return self.mRules[0][0]\n",
    "\n",
    "Grammar.startVar = startVar\n",
    "del startVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `toString` creates a readable presentation of the grammar rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toString(self):\n",
    "    result = ''\n",
    "    for head, *body in self.mRules:\n",
    "        result += f'{head}: {body};\\n'\n",
    "    return result\n",
    "\n",
    "Grammar.__str__ = toString\n",
    "del toString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `EarleyParser` implements the [parsing algorithm of Jay Earley](https://en.wikipedia.org/wiki/Earley_parser).\n",
    "The class maintains the following member variables:\n",
    "- `mGrammar` is the grammar that is used to parse the given token string.\n",
    "- `mString` is the list of tokens and literals that has to be parsed.\n",
    "\n",
    "   As a hack, the first element of this list in `None`.  \n",
    "   Therefore, `mString[i]` is the `i`th token.\n",
    "- `mStateList` is a list of sets of *Earley items*.  If $n$ is the length of the given token string\n",
    "  (excluding the first element `None`), then $Q_i = \\texttt{mStateList}[i]$. \n",
    "  The idea is that the set $Q_i$ is the set of those *Earley items* that the parser could be in \n",
    "  when it has read the tokens `mString[1]`, $\\cdots$,  `mString[n]`.  $Q_0$ is initialized as follows:\n",
    "  $$ Q_0 = \\bigl\\{\\langle\\hat{S} \\rightarrow S \\bullet, 0\\rangle\\bigr\\}. $$\n",
    "  \n",
    "The *Earley items* are interpreted as follows: If we have\n",
    "$$ \\langle C \\rightarrow \\alpha \\bullet \\beta, k\\rangle \\in Q_i, $$\n",
    "then we know the following:\n",
    "- After having read the tokens `mString[:k+1]` the parser tries to parse the variable $C$\n",
    "  in the token string `mString[k+1:]`.\n",
    "- After having read the token string `mString[k+1:i+1]` the parser has already recognized $\\alpha$\n",
    "  and now needs to recognize $\\beta$ in the token string `mString[i+1:]` in order to parse the variable $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser():\n",
    "    def __init__(self, grammar, TokenList):\n",
    "        self.mGrammar   = grammar \n",
    "        self.mString    = [None] + TokenList  # dirty hack so mString[1] is first token\n",
    "        self.mStateList = [set() for i in range(len(TokenList)+1)] \n",
    "        print('Grammar:\\n')\n",
    "        print(self.mGrammar)\n",
    "        print(f'Input: {self.mString}\\n')\n",
    "        self.mStateList[0] = { self.mGrammar.startItem() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `parse` implements Earley's algorithm.  For all states \n",
    "$Q_1$, $\\cdots$, $Q_n$ we proceed as follows:\n",
    "- We apply the completion operation followed by the prediction operation.\n",
    "  This is done until no more states are added to $Q_i$.  \n",
    "  \n",
    "  (The inner `while` loop is not necessary if the grammar does not contain $\\varepsilon$-rules.)\n",
    "- Finally, the scanning operation is applied to $Q_i$.\n",
    "\n",
    "After $Q_i$ has been computed, we proceed to compute $Q_{i+1}$.\n",
    "Parsing is successful iff\n",
    "$$ \\langle\\hat{S} \\rightarrow S \\bullet, 0\\rangle \\in Q_n $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self):\n",
    "    \"run Earley's algorithm\"\n",
    "    n = len(self.mString) - 1 # mString[0] = None\n",
    "    for i in range(0, n+1):\n",
    "        if i + 1 <= n:\n",
    "            next_token = self.mString[i+1]\n",
    "        else:\n",
    "            next_token = 'EOF'\n",
    "        print('_' * 80)\n",
    "        print(f'next token = {next_token}')\n",
    "        print('_' * 80)\n",
    "        change = True\n",
    "        while change:\n",
    "            change = self.complete(i)\n",
    "            change = self.predict(i) or change\n",
    "        self.scan(i)\n",
    "        # print states\n",
    "        print(f'\\nQ{i}:')\n",
    "        Qi = self.mStateList[i]\n",
    "        for item in Qi: \n",
    "            print(item)\n",
    "        if i + 1 <= n:\n",
    "            print(f'\\nQ{i+1}:')\n",
    "            Qip1 = self.mStateList[i+1]\n",
    "            for item in Qip1: \n",
    "                print(item)\n",
    "    if self.mGrammar.finishItem() in self.mStateList[-1]:\n",
    "        print('Parsing successful!')\n",
    "    else:\n",
    "        print('Parsing failed!')\n",
    "\n",
    "EarleyParser.parse = parse\n",
    "del parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `complete(self, i)` applies the completion operation to the state $Q_i$:\n",
    "If we have\n",
    "- $\\langle C \\rightarrow \\gamma \\bullet, j\\rangle \\in Q_i$ and\n",
    "- $\\langle A \\rightarrow \\beta \\bullet C \\delta, k\\rangle \\in Q_j$,\n",
    "then the parser tried to parse the variable $C$ after having read `mString[:j+1]`\n",
    "and we know that \n",
    "$$ C \\Rightarrow^* \\texttt{mString[j+1:i+1]}, $$\n",
    "i.e. the parser has recognized $C$ after having read `mString[j+1:i+1]`.\n",
    "Therefore the parser should proceed to recognize $\\delta$ in state $Q_i$.\n",
    "Therefore we add the *Earley item* $\\langle A \\rightarrow \\beta C \\bullet \\delta,k\\rangle$ to the set $Q_i$:\n",
    "$$\\langle C \\rightarrow \\gamma \\bullet, j\\rangle \\in Q_i \\wedge\n",
    "  \\langle A \\rightarrow \\beta \\bullet C \\delta, k\\rangle \\in Q_j \\;\\rightarrow\\;\n",
    "          Q_i := Q_i \\cup \\bigl\\{ \\langle A \\rightarrow \\beta C \\bullet \\delta, k\\rangle \\bigr\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(self, i):\n",
    "    change = False\n",
    "    added  = True\n",
    "    Qi     = self.mStateList[i]\n",
    "    while added:\n",
    "        added = False\n",
    "        newQi = set()\n",
    "        for item in Qi:\n",
    "            if item.isComplete():\n",
    "                C  = item.mVariable\n",
    "                j  = item.mIndex\n",
    "                Qj = self.mStateList[j]\n",
    "                for newItem in Qj:\n",
    "                    if newItem.sameVar(C):\n",
    "                        moved = newItem.moveDot()\n",
    "                        newQi.add(moved)\n",
    "        if not (newQi <= Qi):\n",
    "            change = True\n",
    "            added  = True\n",
    "            print(\"completion:\")\n",
    "            for newItem in newQi:\n",
    "                if newItem not in Qi:\n",
    "                    print(f'{newItem} added to Q{i}')\n",
    "            self.mStateList[i] |= newQi\n",
    "            Qi = self.mStateList[i]\n",
    "        return change\n",
    "    \n",
    "EarleyParser.complete = complete\n",
    "del complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `self.predict(i)` applies the prediction operation to the state $Q_i$: \n",
    "If $\\langle A \\rightarrow \\beta \\bullet C \\delta, k \\rangle \\in Q_j$, then\n",
    "the parser tries to recognize $C\\delta$ after having read `mString[:j+1]`.  To this end\n",
    "it has to parse $C$ in the string `mString[j+1:]`.\n",
    "Therefore, if $C \\rightarrow \\gamma$ is a rule of our grammar,\n",
    "we add the *Earley item* $\\langle C \\rightarrow \\bullet \\gamma, j\\rangle$ to the set $Q_j$:\n",
    "$$ \\langle A \\rightarrow \\beta \\bullet C \\delta, k\\rangle \\in Q_j \n",
    "       \\wedge (C \\rightarrow \\gamma) \\in R \n",
    "       \\;\\rightarrow\\;\n",
    "       Q_j := Q_j \\cup\\bigl\\{ \\langle C \\rightarrow \\bullet\\gamma, j\\rangle\\bigr\\}.\n",
    "$$\n",
    "As the right hand side $\\gamma$ might start with a variable, the function uses a fix point iteration\n",
    "until no more *Earley items* are added to $Q_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, i):\n",
    "    change = False\n",
    "    added  = True\n",
    "    Qi     = self.mStateList[i]\n",
    "    while added:\n",
    "        added = False\n",
    "        newQi = set()\n",
    "        for item in Qi:\n",
    "            c = item.nextVar()\n",
    "            if c != None:\n",
    "                for rule in self.mGrammar.mRules:\n",
    "                    if c == rule[0]:\n",
    "                        newQi.add(EarleyItem(c, (), rule[1:], i))\n",
    "        if not (newQi <= Qi):\n",
    "            change = True\n",
    "            added  = True\n",
    "            print(\"prediction:\")\n",
    "            for newItem in newQi:\n",
    "                if newItem not in Qi:\n",
    "                    print(f'{newItem} added to Q{i}')\n",
    "            self.mStateList[i] |= newQi\n",
    "            Qi = self.mStateList[i]\n",
    "    return change\n",
    "\n",
    "EarleyParser.predict = predict\n",
    "del predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `self.scan(i)` applies the scanning operation to the state $Q_i$.\n",
    "\n",
    "If $\\langle A \\rightarrow \\beta \\bullet a \\gamma, k\\rangle \\in Q_i$ and $a$ is a token,\n",
    "then the parser tries to recognize the right hand side of the grammar rule\n",
    "$$ A \\rightarrow \\beta a \\gamma$$ \n",
    "and after having read `mString[k+1:i+1]` it has already recognized  $\\beta$.\n",
    "If we now have `mString[i+1] == a`, then the parser still has to recognize $\\gamma$ in `mString[i+2:]`.\n",
    "Therefore, the *Earley object* $\\langle A \\rightarrow \\beta a \\bullet \\gamma, k\\rangle$ is added to\n",
    "the set $Q_{i+1}$:\n",
    "$$\\langle A \\rightarrow \\beta \\bullet a \\gamma, k\\rangle \\in Q_i \\wedge x_{i+1} = a\n",
    "       \\;\\rightarrow\\;\n",
    "       Q_{i+1} := Q_{i+1} \\cup \\bigl\\{ \\langle A \\rightarrow \\beta a \\bullet \\gamma, k\\rangle \\bigr\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(self, i):\n",
    "    Qi = self.mStateList[i]\n",
    "    n  = len(self.mString) - 1 # remember mStateList[0] == None\n",
    "    if i + 1 <= n:\n",
    "        a = self.mString[i+1]\n",
    "        for item in Qi:\n",
    "            if item.scan(a):\n",
    "                self.mStateList[i+1].add(item.moveDot())\n",
    "                print('scanning:')\n",
    "                print(f'{item.moveDot()} added to Q{i+1}')\n",
    "\n",
    "EarleyParser.scan = scan\n",
    "del scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `tokenize` transforms the string `s` into a list of tokens. See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    '''Transform the string s into a list of tokens.  The string s\n",
    "       is supposed to represent an arithmetic expression.\n",
    "    '''\n",
    "    lexSpec = r'''([ \\t]+)        |  # blanks and tabs\n",
    "                  ([1-9][0-9]*|0) |  # number\n",
    "                  ([()])          |  # parentheses \n",
    "                  ([-+*/])        |  # arithmetical operators\n",
    "                  (.)                # unrecognized character\n",
    "               '''\n",
    "    tokenList = re.findall(lexSpec, s, re.VERBOSE)\n",
    "    result    = []\n",
    "    for ws, number, parenthesis, operator, error in tokenList:\n",
    "        if ws:        # skip blanks and tabs\n",
    "            continue\n",
    "        elif number:\n",
    "            result += [ 'NUMBER' ]\n",
    "        elif parenthesis:\n",
    "            result += [ parenthesis ]\n",
    "        elif operator:\n",
    "            result += [ operator ]\n",
    "        else:\n",
    "            result += [ f'ERROR({error})']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('1 + 2 * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `test` takes two arguments.\n",
    "- `file` is the name of a file containing a grammar,\n",
    "- `word` is a string that should be parsed.\n",
    "\n",
    "`word` is first tokenized.  Then the resulting token list is parsed using *Earley's algorithm*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(file, word): \n",
    "    Rules     = parse_grammar(file)\n",
    "    grammar   = Grammar(Rules)\n",
    "    TokenList = tokenize(word)\n",
    "    ep        = EarleyParser(grammar, TokenList)\n",
    "    ep.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test('simple.g', '1 + 2 * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below cleans the directory.  If you are running windows, you have to replace `rm`with `del`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm GrammarLexer.* GrammarParser.* Grammar.tokens GrammarListener.py Grammar.interp\n",
    "!rm -r __pycache__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
