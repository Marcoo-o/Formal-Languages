{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".container { width: 100% }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<style>\n",
    ".container { width: 100% }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tokenizer for numbers and the arithmetical operators `+`, `-`, `*`, `/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a definition of the *token names*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = (\n",
    "   'NUMBER',\n",
    "   'PLUS',\n",
    "   'MINUS',\n",
    "   'TIMES',\n",
    "   'DIVIDE',\n",
    "   'LPAREN',\n",
    "   'RPAREN',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define regular expressions that define the tokens that are to be recognized.\n",
    "Note that some operators have to be prefixed with a backslash since these operators are\n",
    "also used as operators for regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_PLUS    = r'\\+'\n",
    "t_MINUS   = r'-'\n",
    "t_TIMES   = r'\\*'\n",
    "t_DIVIDE  = r'/'\n",
    "t_LPAREN  = r'\\('\n",
    "t_RPAREN  = r'\\)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular expression rule with some action code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_NUMBER(t):\n",
    "    r'0|[1-9][0-9]*'\n",
    "    t.value = int(t.value)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a rule so we can track line numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_newline(t):\n",
    "    r'\\n+'\n",
    "    t.lexer.lineno += len(t.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A string containing ignored characters (spaces and tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ignore  = ' \\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error handling rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_error(t):\n",
    "    print(\"Illegal character '%s'\" % t.value[0])\n",
    "    t.lexer.skip(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the lexer.  Since this code is expected to be part of some python file but really isn't, \n",
    "we have to set the variable `__file__` manually to fool the system into believing that the code \n",
    "given above is located in a file called `hugo.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'hugo'\n",
    "lexer = lex.lex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test it with the following string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \\\n",
    "'''\n",
    "3 + 4 * 10 + 007\n",
    "  + (-20)*2 abc\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us feed the lexier with the string `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer.input(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the lexer to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(NUMBER,3,2,1)\n",
      "Illegal character ' '\n",
      "LexToken(PLUS,'+',2,3)\n",
      "Illegal character ' '\n",
      "LexToken(NUMBER,4,2,5)\n",
      "Illegal character ' '\n",
      "LexToken(TIMES,'*',2,7)\n",
      "Illegal character ' '\n",
      "LexToken(NUMBER,10,2,9)\n",
      "Illegal character ' '\n",
      "LexToken(PLUS,'+',2,12)\n",
      "Illegal character ' '\n",
      "LexToken(NUMBER,0,2,14)\n",
      "LexToken(NUMBER,0,2,15)\n",
      "LexToken(NUMBER,7,2,16)\n",
      "Illegal character ' '\n",
      "Illegal character ' '\n",
      "LexToken(PLUS,'+',3,20)\n",
      "Illegal character ' '\n",
      "LexToken(LPAREN,'(',3,22)\n",
      "LexToken(MINUS,'-',3,23)\n",
      "LexToken(NUMBER,20,3,24)\n",
      "LexToken(RPAREN,')',3,26)\n",
      "LexToken(TIMES,'*',3,27)\n",
      "LexToken(NUMBER,2,3,28)\n",
      "Illegal character ' '\n",
      "Illegal character 'a'\n",
      "Illegal character 'b'\n",
      "Illegal character 'c'\n"
     ]
    }
   ],
   "source": [
    "for tok in lexer:\n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
