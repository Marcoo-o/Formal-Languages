{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(open('../style.css').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an Earley Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat simple.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "grammar\n",
    "    : rule\n",
    "    | rule grammar\n",
    "    ;\n",
    "\n",
    "rule\n",
    "    : VARIABLE ':' item_list ';' \n",
    "    ;\n",
    "\n",
    "item_list\n",
    "    : λ\n",
    "    | item item_list\n",
    "    ;\n",
    "\n",
    "item \n",
    "    : VARIABLE \n",
    "    | TERMINAL   \n",
    "    | LITERAL\n",
    "    ;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.lex as lex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ 'VARIABLE',  # r'[a-z][a-z0-9_]*'\n",
    "           'TERMINAL',  # r'[A-Z][A-Z0-9_]*'\n",
    "           'LITERAL',   # r\"'.'\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_VARIABLE = r'[a-z][a-z0-9_]*'\n",
    "t_TERMINAL = r'[A-Z][A-Z0-9_]*'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A literal is any character that is not the newline character and is enclosed in quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_LITERAL(t):\n",
    "    r\"'.'\"\n",
    "    t.value = t.value[1]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literals = [':', ';']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ignore = ' \\t\\r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_newline(t):\n",
    "    r'\\n'\n",
    "    t.lexer.lineno += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_column(token):\n",
    "    program    = token.lexer.lexdata  # the complete string given to the scanner\n",
    "    line_start = program.rfind('\\n', 0, token.lexpos)\n",
    "    return token.lexpos - line_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_error(t):\n",
    "    column = find_column(t)\n",
    "    print(f\"Illegal character '{t.value[0]}' in line {t.lineno}, column {column}.\")\n",
    "    t.lexer.skip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'main'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexer = lex.lex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_scanner(file_name):\n",
    "    with open(file_name, 'r') as handle:\n",
    "        program = handle.read() \n",
    "    print(program)\n",
    "    lexer.input(program)\n",
    "    lexer.lineno = 1          # reset line number\n",
    "    for t in lexer:           # start scanning and collect all tokens\n",
    "        print(t) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scanner('simple.g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the following grammar to describe context free grammars:\n",
    "```\n",
    "    grammar\n",
    "        : rule\n",
    "        | rule grammar\n",
    "        ;\n",
    "\n",
    "    rule\n",
    "        : VARIABLE ':' item_list ';'\n",
    "        ;\n",
    "\n",
    "    item_list\n",
    "        : λ\n",
    "        | item item_list\n",
    "        ;\n",
    "\n",
    "    item\n",
    "        : VARIABLE\n",
    "        | TERMINAL\n",
    "        | LITERAL\n",
    "        ;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ply.yacc as yacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 'grammar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grammar is represented as a list of grammar rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_grammar_one(p):\n",
    "    \"grammar : rule\"\n",
    "    p[0] = [p[1]]\n",
    "\n",
    "def p_grammar_more(p):\n",
    "    \"grammar : rule grammar\"\n",
    "    p[0] = [p[1]] + p[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grammar rule is represented as a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_rule(p):\n",
    "    \"rule : VARIABLE ':' item_list ';'\"\n",
    "    p[0] = [p[1]] + p[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_item_list_zero(p):\n",
    "    \"item_list : \"\n",
    "    p[0] = []\n",
    "\n",
    "def p_item_list_more(p):\n",
    "    \"item_list : item item_list\"\n",
    "    p[0] = [p[1]] + p[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Items are strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_item_variable(p):\n",
    "    \"item : VARIABLE\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_item_terminal(p):\n",
    "    \"item : TERMINAL\"\n",
    "    p[0] = p[1]\n",
    "\n",
    "def p_item_literal(p):\n",
    "    \"item : LITERAL\"\n",
    "    p[0] = p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_error(t):\n",
    "    column = find_column(t)\n",
    "    if t:\n",
    "        print(f'Syntax error at token \"{t.value}\" in line {t.lineno}, column {column}.')\n",
    "    else:\n",
    "        print('Syntax error at end of input.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yacc.yacc(write_tables=False, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(file):\n",
    "    lexer.lineno = 1\n",
    "    with open(file, 'r') as handle:\n",
    "        grammar = handle.read() \n",
    "    print(grammar)\n",
    "    ast = yacc.parse(grammar)\n",
    "    return ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse('simple.g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_mypy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Earley's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a context-free grammar $G = \\langle V, \\Sigma, R, s \\rangle$ and a string $w = x_1x_2 \\cdots x_n \\in \\Sigma^*$ of length $n$, \n",
    "an *Earley item* is a pair of the form\n",
    "$$\\langle a \\rightarrow \\alpha \\bullet \\beta, k \\rangle$$\n",
    "such that \n",
    "- $(a \\rightarrow \\alpha \\beta) \\in R\\quad$  and\n",
    "- $k \\in \\{0,1,\\cdots,n\\}$. \n",
    "\n",
    "The class `EarleyItem` represents a single *Earley item*.  \n",
    "- `mVariable` is the variable $a$,\n",
    "- `mAlpha` is $\\alpha$,\n",
    "- `mBeta` is $\\beta$, and\n",
    "- `mIndex` is $k$.\n",
    "\n",
    "Since we later have to store objects of class `EarleyItem` in sets, we have to implement the functions\n",
    "- `__eq__`,\n",
    "- `__ne__`,\n",
    "- `__hash__`.\n",
    "\n",
    "It is easiest to implement `__hash__` by first converting the object into a string.  Hence we also\n",
    "implement the function `__repr__`, that converts an `EarleyItem` into a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next import is needed for the type checker, because the class `EarleyItem` is used in the signature of `moveDot` and at this point the class `EarleyItem` is not yet defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyItem():\n",
    "    def __init__(self, variable: str, alpha: tuple[str, ...], beta: tuple[str, ...], index: int) -> None:\n",
    "        self.mVariable = variable\n",
    "        self.mAlpha    = alpha\n",
    "        self.mBeta     = beta\n",
    "        self.mIndex    = index\n",
    "    \n",
    "    def __eq__(self, other: object) -> bool:\n",
    "        return isinstance(other, EarleyItem)     and \\\n",
    "               self.mVariable == other.mVariable and \\\n",
    "               self.mAlpha    == other.mAlpha    and \\\n",
    "               self.mBeta     == other.mBeta     and \\\n",
    "               self.mIndex    == other.mIndex\n",
    "    \n",
    "    def __ne__(self, other: object):\n",
    "        return not self.__eq__(other)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.__repr__())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        alphaStr = ' '.join(self.mAlpha)\n",
    "        betaStr  = ' '.join(self.mBeta)\n",
    "        return f'<{self.mVariable} → {alphaStr} • {betaStr}, {self.mIndex}>'\n",
    "\n",
    "    # The following methods are stubs that are necessary for the type checker.\n",
    "    def isComplete(self) -> bool:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def sameVar(self, c: str) -> bool:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def scan(self, T: str) -> bool:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def nextVar(self) -> str | None:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def moveDot(self) -> EarleyItem: # type: ignore\n",
    "        return None # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an Earley item `self`, the function `isComplete` checks, whether the Earley item `self` has the form\n",
    "$$\\langle A \\rightarrow \\alpha \\bullet, k \\rangle,$$\n",
    "i.e. whether the $\\bullet$ is at the end of the grammar rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isComplete(self: EarleyItem) -> bool:\n",
    "    return self.mBeta == ()\n",
    "\n",
    "EarleyItem.isComplete = isComplete # type: ignore\n",
    "del isComplete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `sameVar(self, c)` checks, whether the item following the dot is the same as the variable \n",
    "given as argument, i.e. `sameVar(self, c)` returns `True` if `self` is an Earley item of the form\n",
    "$$\\langle a \\rightarrow \\alpha \\bullet c\\beta, k \\rangle.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sameVar(self, c: str) -> bool:\n",
    "    return len(self.mBeta) > 0 and self.mBeta[0] == c\n",
    "\n",
    "EarleyItem.sameVar = sameVar # type: ignore\n",
    "del sameVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `scan(self, T)` checks, whether the item following the dot matches the token `T`, \n",
    "i.e. `scan(self, T)` returns `True` if `self` is an Earley item of the form\n",
    "$$\\langle a \\rightarrow \\alpha \\bullet T\\beta, k \\rangle.$$\n",
    "The argument $T$ can either be the name of a token or a literal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(self, T: str) -> bool:\n",
    "    if len(self.mBeta) > 0:\n",
    "        return self.mBeta[0] == T or self.mBeta[0] == \"'\" + T + \"'\"\n",
    "    return False\n",
    "\n",
    "EarleyItem.scan = scan # type: ignore\n",
    "del scan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an Earley item, this function returns the name of the variable following the dot.  If there is no variable following the dot, the function returns `None`.  The function can distinguish variables from token names because variable names consist only of lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextVar(self) -> str | None:\n",
    "    if len(self.mBeta) > 0:\n",
    "        var = self.mBeta[0]\n",
    "        if var[0] != \"'\" and var.islower():\n",
    "            return var\n",
    "    return None\n",
    "\n",
    "EarleyItem.nextVar = nextVar # type: ignore\n",
    "del nextVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `moveDot(self)` moves the $\\bullet$ in the Earley item `self`, where `self` has the form \n",
    "$$\\langle a \\rightarrow \\alpha \\bullet \\beta, k \\rangle$$\n",
    "over the next variable, token, or literal in $\\beta$.  It assumes that $\\beta$ is not empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveDot(self) -> EarleyItem:\n",
    "    return EarleyItem(self.mVariable, \n",
    "                      self.mAlpha + (self.mBeta[0],), \n",
    "                      self.mBeta[1:], \n",
    "                      self.mIndex)\n",
    "\n",
    "EarleyItem.moveDot = moveDot # type: ignore\n",
    "del moveDot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Grammar` represents a context free grammar.  It stores a list of the rules of the grammar.\n",
    "Each grammar rule of the form\n",
    "$$ a \\rightarrow \\beta $$\n",
    "is stored as the tuple $(a,) + \\beta$.  The start symbol is assumed to be the variable on the left hand side of\n",
    "the first rule. To distinguish syntactical variables from tokens, variables contain only lower case letters,\n",
    "while tokens either contain only upper case letters or they start and end with a single quote character \"`'`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grammar():\n",
    "    def __init__(self, Rules: list[list[str]]):\n",
    "        self.mRules = Rules   \n",
    "\n",
    "    def startItem(self) -> EarleyItem:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def finishItem(self) -> EarleyItem:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def startVar(self) -> str:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def toString(self) -> str:\n",
    "        return None # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `startItem` returns the Earley item\n",
    "$$ \\langle\\hat{s} \\rightarrow \\bullet s, 0\\rangle $$\n",
    "where $s$ is the start variable of the given grammar and $\\hat{s}$ is the new variable `Start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startItem(self) -> EarleyItem:\n",
    "    return EarleyItem('Start', (), (self.startVar(),), 0)\n",
    "\n",
    "Grammar.startItem = startItem # type: ignore\n",
    "del startItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `finishItem` returns the Earley item\n",
    "$$ \\langle\\hat{s} \\rightarrow s \\bullet, 0\\rangle $$\n",
    "where $s$ is the start variable of the given grammar and $\\hat{s}$ is a new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishItem(self) -> EarleyItem:\n",
    "    return EarleyItem('Start', (self.startVar(),), (), 0)\n",
    "\n",
    "Grammar.finishItem = finishItem # type: ignore\n",
    "del finishItem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `startVar` returns the start variable of the grammar.  It is assumed that\n",
    "the first rule grammar starts with the start variable of the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def startVar(self) -> str:\n",
    "    return self.mRules[0][0]\n",
    "\n",
    "Grammar.startVar = startVar # type: ignore\n",
    "del startVar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `toString` creates a readable presentation of the grammar rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toString(self) -> str:\n",
    "    result = ''\n",
    "    for head, *body in self.mRules:\n",
    "        result += f'{head}: {body};\\n'\n",
    "    return result\n",
    "\n",
    "Grammar.__str__ = toString # type: ignore\n",
    "del toString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `EarleyParser` implements the [parsing algorithm of Jay Earley](https://en.wikipedia.org/wiki/Earley_parser).\n",
    "The class maintains the following member variables:\n",
    "- `mGrammar` is the grammar that is used to parse the given token string.\n",
    "- `mString` is the list of tokens and literals that has to be parsed.\n",
    "\n",
    "   As a hack, the first element of this list in `None`.  \n",
    "   Therefore, `mString[i]` is the $i^\\textrm{th}$ token.\n",
    "- `mStateList` is a list of sets of *Earley items*.  If $n$ is the length of the given token string\n",
    "  (excluding the first element `None`), then $Q_i = \\texttt{mStateList}[i]$. \n",
    "  The idea is that the set $Q_i$ is the set of those *Earley items* that the parser could be in \n",
    "  when it has read the tokens `mString[1]`, $\\cdots$,  `mString[i]`.  $Q_0$ is initialized as follows:\n",
    "  $$ Q_0 = \\bigl\\{\\langle\\hat{s} \\rightarrow \\bullet s, 0\\rangle\\bigr\\}. $$\n",
    "  \n",
    "The *Earley items* are interpreted as follows: If we have\n",
    "$$ \\langle c \\rightarrow \\alpha \\bullet \\beta, k\\rangle \\in Q_i, $$\n",
    "then we know the following:\n",
    "- After having read the tokens `mString[:k+1]` the parser tries to parse the variable $c$\n",
    "  in the token string `mString[k+1:]`.\n",
    "- After having read the token string `mString[k+1:i+1]` the parser has already recognized $\\alpha$\n",
    "  and now needs to recognize $\\beta$ in the token string `mString[i+1:]` in order to parse the variable $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser():\n",
    "    def __init__(self, grammar, TokenList):\n",
    "        self.mGrammar   = grammar \n",
    "        self.mString    = [None] + TokenList  # hack so mString[1] is the first token\n",
    "        self.mStateList = [set() for i in range(len(TokenList)+1)] \n",
    "        print('Grammar:\\n')\n",
    "        print(self.mGrammar)\n",
    "        print(f'Input: {self.mString}\\n')\n",
    "        self.mStateList[0] = { self.mGrammar.startItem() }\n",
    "\n",
    "    def parse(self) -> None:\n",
    "        return None\n",
    "\n",
    "    def complete(self, i: int) -> bool:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def predict(self, i: int) -> bool:\n",
    "        return None # type: ignore\n",
    "\n",
    "    def scan(self, i: int) -> None:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `parse` implements Earley's algorithm.  For all states \n",
    "$Q_1$, $\\cdots$, $Q_n$ we proceed as follows:\n",
    "- We apply the *completion* operation followed by the *prediction* operation.\n",
    "  This is done until no more states are added to $Q_i$.  \n",
    "  \n",
    "  (The inner `while` loop is not necessary if the grammar does not contain $\\varepsilon$-rules.)\n",
    "- Finally, the *scanning* operation is applied to $Q_i$.  This operation adds\n",
    "  items to the set $Q_{i+1}$.\n",
    "\n",
    "After $Q_i$ has been computed, we proceed to process $Q_{i+1}$.\n",
    "Parsing is successful iff\n",
    "$$ \\langle\\hat{s} \\rightarrow s \\bullet, 0\\rangle \\in Q_n $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def earley_parse(self) -> None:\n",
    "    \"run Earley's algorithm\"\n",
    "    print(\"starting...\")\n",
    "    n = len(self.mString) - 1 # mString[0] = None\n",
    "    for i in range(0, n+1):\n",
    "        if i + 1 <= n:\n",
    "            next_token = self.mString[i+1]\n",
    "        else:\n",
    "            next_token = 'EOF'\n",
    "        print('_' * 80)\n",
    "        print(f'next token = {next_token}')\n",
    "        print('_' * 80)\n",
    "        change = True\n",
    "        while change:\n",
    "            change = self.complete(i)\n",
    "            change = self.predict(i) or change\n",
    "        self.scan(i)\n",
    "        # print state\n",
    "        print(f'\\nQ{i}:')\n",
    "        Qi = self.mStateList[i]\n",
    "        for item in Qi: \n",
    "            print(item)\n",
    "        if i + 1 <= n:\n",
    "            print(f'\\nQ{i+1}:')\n",
    "            Qip1 = self.mStateList[i+1]\n",
    "            for item in Qip1: \n",
    "                print(item)\n",
    "    if self.mGrammar.finishItem() in self.mStateList[-1]:\n",
    "        print('Parsing successful!')\n",
    "    else:\n",
    "        print('Parsing failed!')\n",
    "\n",
    "EarleyParser.parse = earley_parse # type: ignore\n",
    "del earley_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `complete(self, i)` applies the completion operation to the state $Q_i$:\n",
    "If we have\n",
    "- $\\langle c \\rightarrow \\gamma \\bullet, j\\rangle \\in Q_i$ and\n",
    "- $\\langle a \\rightarrow \\beta \\bullet c \\delta, k\\rangle \\in Q_j$,\n",
    "then the parser tried to parse the variable $c$ after having read `mString[:j+1]`\n",
    "and we know that \n",
    "$$ c \\Rightarrow^* \\texttt{mString[j+1:i+1]}, $$\n",
    "i.e. the parser has recognized $c$ after having read `mString[j+1:i+1]`.\n",
    "Therefore the parser should proceed to recognize $\\delta$ in state $Q_i$.\n",
    "Therefore we add the *Earley item* $\\langle a \\rightarrow \\beta c \\bullet \\delta,k\\rangle$ to the set $Q_i$:\n",
    "$$\\langle c \\rightarrow \\gamma \\bullet, j\\rangle \\in Q_i \\wedge\n",
    "  \\langle a \\rightarrow \\beta \\bullet c \\delta, k\\rangle \\in Q_j \\;\\rightarrow\\;\n",
    "          Q_i := Q_i \\cup \\bigl\\{ \\langle a \\rightarrow \\beta c \\bullet \\delta, k\\rangle \\bigr\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(self, i: int) -> bool:\n",
    "    change = False\n",
    "    added  = True\n",
    "    Qi     = self.mStateList[i]\n",
    "    while added:\n",
    "        added = False\n",
    "        newQi = set()\n",
    "        for item in Qi:\n",
    "            if item.isComplete():\n",
    "                C  = item.mVariable\n",
    "                j  = item.mIndex\n",
    "                Qj = self.mStateList[j]\n",
    "                for newItem in Qj:\n",
    "                    if newItem.sameVar(C):\n",
    "                        moved = newItem.moveDot()\n",
    "                        newQi.add(moved)\n",
    "        if not (newQi <= Qi):\n",
    "            change = True\n",
    "            added  = True\n",
    "            print(\"completion:\")\n",
    "            for newItem in newQi:\n",
    "                if newItem not in Qi:\n",
    "                    print(f'{newItem} added to Q{i}')\n",
    "            self.mStateList[i] |= newQi\n",
    "            Qi = self.mStateList[i]\n",
    "    return change\n",
    "    \n",
    "EarleyParser.complete = complete # type: ignore\n",
    "del complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `self.predict(i)` applies the prediction operation to the state $Q_i$: \n",
    "If $\\langle a \\rightarrow \\beta \\bullet c \\delta, k \\rangle \\in Q_j$, then\n",
    "the parser tries to recognize $c\\delta$ after having read `mString[:j+1]`.  To this end\n",
    "it has to parse $c$ in the string `mString[j+1:]`.\n",
    "Therefore, if $c \\rightarrow \\gamma$ is a rule of our grammar,\n",
    "we add the *Earley item* $\\langle c \\rightarrow \\bullet \\gamma, j\\rangle$ to the set $Q_j$:\n",
    "$$ \\langle a \\rightarrow \\beta \\bullet c \\delta, k\\rangle \\in Q_j \n",
    "       \\wedge (c \\rightarrow \\gamma) \\in R \n",
    "       \\;\\rightarrow\\;\n",
    "       Q_j := Q_j \\cup\\bigl\\{ \\langle c \\rightarrow \\bullet\\gamma, j\\rangle\\bigr\\}.\n",
    "$$\n",
    "As the right hand side $\\gamma$ might start with a variable, the function uses a fix point iteration\n",
    "until no more *Earley items* are added to $Q_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, i: int) -> bool:\n",
    "    change = False\n",
    "    added  = True\n",
    "    Qi     = self.mStateList[i]\n",
    "    while added:\n",
    "        added = False\n",
    "        newQi = set()\n",
    "        for item in Qi:\n",
    "            c = item.nextVar()\n",
    "            if c != None:\n",
    "                for rule in self.mGrammar.mRules:\n",
    "                    if c == rule[0]:\n",
    "                        newQi.add(EarleyItem(c, (), tuple(rule[1:]), i))\n",
    "        if not (newQi <= Qi):\n",
    "            change = True\n",
    "            added  = True\n",
    "            print(\"prediction:\")\n",
    "            for newItem in newQi:\n",
    "                if newItem not in Qi:\n",
    "                    print(f'{newItem} added to Q{i}')\n",
    "            self.mStateList[i] |= newQi\n",
    "            Qi = self.mStateList[i]\n",
    "    return change\n",
    "\n",
    "EarleyParser.predict = predict # type: ignore\n",
    "del predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `self.scan(i)` applies the scanning operation to the state $Q_i$.\n",
    "\n",
    "If $\\langle a \\rightarrow \\beta \\bullet T \\gamma, k\\rangle \\in Q_i$ and $T$ is a token,\n",
    "then the parser tries to recognize the right hand side of the grammar rule\n",
    "$$ a \\rightarrow \\beta T \\gamma$$ \n",
    "and after having read `mString[k+1:i+1]` it has already recognized  $\\beta$.\n",
    "If we now have `mString[i+1] == a`, then the parser still has to recognize $\\gamma$ in `mString[i+2:]`.\n",
    "Therefore, the *Earley object* $\\langle a \\rightarrow \\beta T \\bullet \\gamma, k\\rangle$ is added to\n",
    "the set $Q_{i+1}$:\n",
    "$$\\langle a \\rightarrow \\beta \\bullet T \\gamma, k\\rangle \\in Q_i \\wedge x_{i+1} = T\n",
    "       \\;\\rightarrow\\;\n",
    "       Q_{i+1} := Q_{i+1} \\cup \\bigl\\{ \\langle a \\rightarrow \\beta T \\bullet \\gamma, k\\rangle \\bigr\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan(self, i: int) -> None:\n",
    "    Qi = self.mStateList[i]\n",
    "    n  = len(self.mString) - 1 # remember mStateList[0] == None\n",
    "    if i + 1 <= n:\n",
    "        a = self.mString[i+1]\n",
    "        for item in Qi:\n",
    "            if item.scan(a):\n",
    "                self.mStateList[i+1].add(item.moveDot())\n",
    "                print('scanning:')\n",
    "                print(f'{item.moveDot()} added to Q{i+1}')\n",
    "\n",
    "EarleyParser.scan = scan # type: ignore\n",
    "del scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `tokenize` transforms the string `s` that is to be parsed into a list of tokens. See below for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s: str) -> list[str]:\n",
    "    '''Transform the string s into a list of tokens.  The string s\n",
    "       is supposed to represent an arithmetic expression.\n",
    "    '''\n",
    "    lexSpec = r'''([ \\t]+)        |  # blanks and tabs\n",
    "                  ([1-9][0-9]*|0) |  # number\n",
    "                  ([()])          |  # parentheses \n",
    "                  ([-+*/])        |  # arithmetical operators\n",
    "                  (.)                # unrecognized character\n",
    "               '''\n",
    "    tokenList = re.findall(lexSpec, s, re.VERBOSE)\n",
    "    result    = []\n",
    "    for ws, number, parenthesis, operator, error in tokenList:\n",
    "        if ws:        # skip blanks and tabs\n",
    "            continue\n",
    "        elif number:\n",
    "            result += [ 'NUMBER' ]\n",
    "        elif parenthesis:\n",
    "            result += [ parenthesis ]\n",
    "        elif operator:\n",
    "            result += [ operator ]\n",
    "        else:\n",
    "            result += [ f'ERROR({error})']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize('1 + 2 * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `test` takes two arguments.\n",
    "- `file` is the name of a file containing a grammar,\n",
    "- `word` is a string that should be parsed.\n",
    "\n",
    "`word` is first tokenized.  Then the resulting token list is parsed using *Earley's algorithm*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(file: str, word: str) -> None: \n",
    "    lexer.lineno = 1 # type: ignore\n",
    "    with open(file, 'r') as handle:\n",
    "        grammarStr = handle.read() \n",
    "    print(grammarStr)\n",
    "    Rules = yacc.parse(grammarStr) # type: ignore\n",
    "    grammar   = Grammar(Rules)\n",
    "    TokenList = tokenize(word)\n",
    "    ep        = EarleyParser(grammar, TokenList)\n",
    "    ep.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test('simple.g', '1 + 2 * 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
